# Concepts fondamentaux : LLM, MCP, RAG

**Format** : PrÃ©sentation rapide, approfondissement durant les exercices

---

## ğŸ¯ Objectif

Comprendre les concepts essentiels pour travailler efficacement avec l'IA gÃ©nÃ©rative :

- LLM, tokens, tempÃ©rature, contexte
- Prompting efficace
- Model Context Protocol (MCP)
- RAG (Retrieval Augmented Generation)
- Embeddings et recherche sÃ©mantique

---

## ğŸ§  LLM : Large Language Models

### Qu'est-ce qu'un LLM ?

**DÃ©finition** : ModÃ¨le de machine learning entraÃ®nÃ© sur d'immenses quantitÃ©s de texte pour prÃ©dire la suite la plus probable.

**ModÃ¨les populaires (2025)** :

| ModÃ¨le                            | Taille contexte | Forces                                   | Cas d'usage dev             |
| --------------------------------- | --------------- | ---------------------------------------- | --------------------------- |
| **GPT-4o** (OpenAI)               | 128k tokens     | Polyvalent, rapide                       | Code gÃ©nÃ©ral, debugging     |
| **Claude 3.5 Sonnet** (Anthropic) | 200k tokens     | Excellent en code, suit les instructions | Refactoring, architecture   |
| **Gemini 1.5 Pro** (Google)       | 2M tokens       | Contexte Ã©norme                          | Analyse de codebase entiÃ¨re |
| **o1** (OpenAI)                   | 128k tokens     | Raisonnement profond                     | ProblÃ¨mes complexes         |

---

### Tokens : L'unitÃ© de mesure

**Qu'est-ce qu'un token ?**

- Morceau de texte (mot, partie de mot, caractÃ¨re)
- **~4 caractÃ¨res = 1 token** (en moyenne, pour l'anglais)
- **~3 caractÃ¨res = 1 token** (pour le code)

**Exemples** :

```
"Hello world" = 2 tokens
"function calculateTotal()" = 5 tokens
"$this->reviewRepository->findByAgencyId($agencyId)" = ~12 tokens
```

**Pourquoi c'est important ?**

- ğŸ’° **CoÃ»t** : Facturation par token (input + output)
- â±ï¸ **Latence** : Plus de tokens = rÃ©ponse plus lente
- ğŸ§  **Limite de contexte** : Max 128k-2M tokens selon modÃ¨le

**Tarifs indicatifs (2025)** :

| ModÃ¨le            | Input (par 1M tokens) | Output (par 1M tokens) |
| ----------------- | --------------------- | ---------------------- |
| GPT-4o            | $2.50                 | $10.00                 |
| Claude 3.5 Sonnet | $3.00                 | $15.00                 |
| Gemini 1.5 Pro    | $1.25                 | $5.00                  |

---

### TempÃ©rature : ContrÃ´ler la crÃ©ativitÃ©

**DÃ©finition** : ParamÃ¨tre qui contrÃ´le l'alÃ©atoire des rÃ©ponses (0 Ã  2)

| TempÃ©rature   | Comportement             | Quand l'utiliser                                      |
| ------------- | ------------------------ | ----------------------------------------------------- |
| **0 - 0.2**   | DÃ©terministe, prÃ©visible | **Code production**, gÃ©nÃ©ration de tests, refactoring |
| **0.5 - 0.7** | Ã‰quilibrÃ©                | Brainstorming, suggestions d'architecture             |
| **0.8 - 1.5** | CrÃ©atif, variÃ©           | Noms de variables, documentation, idÃ©es               |

**ğŸ’¡ Recommandation pour le code** : **TempÃ©rature 0** â†’ ReproductibilitÃ© maximale

---

### Contexte : La fenÃªtre de mÃ©moire

**Contexte** = Tout ce que l'IA "voit" :

- Votre prompt
- L'historique de conversation
- Les fichiers ouverts (dans IDE IA)
- Les rÃ©sultats de tool calls (MCP)

**Limite de contexte** :

- GPT-4o : 128k tokens â‰ˆ 300 pages
- Claude 3.5 Sonnet : 200k tokens â‰ˆ 500 pages
- Gemini 1.5 Pro : 2M tokens â‰ˆ 5000 pages

**ProblÃ¨me** : Une fois le contexte dÃ©passÃ©, l'IA "oublie" le dÃ©but

**Solutions** :

- RÃ©sumer les parties anciennes
- Utiliser RAG pour rÃ©cupÃ©rer seulement l'info pertinente
- DÃ©couper en conversations plus courtes

---

## ğŸ’¬ Prompting efficace

### Anatomie d'un bon prompt

**Structure recommandÃ©e** :

```markdown
## Contexte

[DÃ©crire le contexte du projet, la stack technique, les contraintes]

## TÃ¢che

[DÃ©crire clairement ce que l'IA doit faire]

## Contraintes

[Lister les rÃ¨gles Ã  respecter : standards de code, architecture, librairies]

## Exemples (optionnel)

[Montrer des exemples de code existant pour guider le style]

## Questions

[Inviter l'IA Ã  poser des questions si besoin de clarifications]
```

**Exemple concret** :

```markdown
## Contexte

Je dÃ©veloppe une API de gestion d'avis avec Symfony 6.4 + API Platform 3.2.
Architecture : Controller â†’ Service â†’ Repository.

## TÃ¢che

CrÃ©e un service ReviewService avec une mÃ©thode createReview() qui :

- Valide les donnÃ©es (rating 1-5, comment min 10 caractÃ¨res)
- VÃ©rifie qu'un avis n'existe pas dÃ©jÃ  pour cet utilisateur et cette agence
- Persiste l'avis en base de donnÃ©es
- Retourne le DTO ReviewDto

## Contraintes

- PHP 8.2 strict typing
- Utilise Symfony Validator
- Lance ReviewAlreadyExistsException si duplicata
- Lance ValidationException si validation Ã©choue
- Pas de logique dans le Controller

## Questions

As-tu besoin de voir le schÃ©ma de base de donnÃ©es ou les DTOs existants ?
```

---

### Techniques de prompting avancÃ©es

**1. Role prompting** (dÃ©finir un rÃ´le)

```markdown
Tu es un dÃ©veloppeur senior PHP/Symfony avec 10 ans d'expÃ©rience.
Tu suis strictement les principes SOLID et les PSR-12.
```

**2. Chain-of-thought** (demander le raisonnement)

```markdown
Avant de gÃ©nÃ©rer le code, explique ton approche :

1. Quelle architecture vas-tu utiliser ?
2. Quels patterns vas-tu appliquer ?
3. Quels cas d'erreur dois-tu gÃ©rer ?

Ensuite, gÃ©nÃ¨re le code.
```

**3. Few-shot examples** (donner des exemples)

```markdown
Voici un exemple de service existant dans notre codebase :

[coller un exemple de bon code]

GÃ©nÃ¨re un service similaire pour les avis, en suivant le mÃªme style.
```

**4. Contraintes nÃ©gatives** (dire ce qu'on NE veut PAS)

```markdown
## Ce que tu NE dois PAS faire :

- âŒ Pas de code dans le Controller
- âŒ Pas de requÃªtes SQL brutes
- âŒ Pas de foreach si on peut utiliser une mÃ©thode de Collection
- âŒ Pas de commentaires Ã©vidents
```

**5. ItÃ©rations guidÃ©es**

```markdown
Ã‰tape 1 : Propose la signature de la mÃ©thode avec type hints
â†’ [validation par le dev]

Ã‰tape 2 : GÃ©nÃ¨re l'implÃ©mentation
â†’ [validation par le dev]

Ã‰tape 3 : Ajoute la gestion d'erreurs
```

---

## ğŸ”Œ MCP : Model Context Protocol

### Qu'est-ce que MCP ?

**MCP** = Protocole standardisÃ© pour connecter des **outils externes** aux LLMs.

**Analogie** : MCP est aux LLMs ce que les APIs REST sont aux applications web.

**Cas d'usage** :

- AccÃ©der Ã  des bases de donnÃ©es
- Lire/Ã©crire des fichiers
- Appeler des APIs externes
- RÃ©cupÃ©rer des donnÃ©es depuis JIRA, Figma, Slack, etc.
- ExÃ©cuter des commandes systÃ¨me

---

### Architecture MCP

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Claude    â”‚
â”‚   (LLM)     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ MCP Protocol
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
â”‚ MCP Server  â”‚
â”‚  (Node.js   â”‚
â”‚   ou Python)â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€ Database (SQLite, PostgreSQL, etc.)
       â”œâ”€â”€â”€ APIs (JIRA, Figma, GitHub, etc.)
       â”œâ”€â”€â”€ Filesystem
       â””â”€â”€â”€ Other tools
```

**Composants** :

- **MCP Server** : Expose des "resources" et des "tools"
- **LLM Client** : Appelle les tools du serveur
- **Protocol** : JSON-RPC standardisÃ©

---

### Exemple : Serveur MCP pour avis

**Ce que vous allez construire dans l'exercice 2** :

```json
{
  "mcpServers": {
    "reviews-db": {
      "command": "node",
      "args": ["./mcp-server/index.js"],
      "tools": [
        {
          "name": "search_reviews",
          "description": "Recherche sÃ©mantique dans les avis",
          "parameters": {
            "query": "string",
            "limit": "number"
          }
        },
        {
          "name": "get_stats",
          "description": "Statistiques sur les avis",
          "parameters": {
            "agencyId": "string (optional)"
          }
        }
      ]
    }
  }
}
```

**Workflow** :

1. Vous demandez Ã  Claude : "Quels sont les avis positifs sur l'agence X ?"
2. Claude appelle le tool `search_reviews` via MCP
3. Le serveur MCP interroge la base vectorielle
4. Les rÃ©sultats remontent Ã  Claude
5. Claude vous rÃ©pond en langage naturel

---

## ğŸ” RAG : Retrieval Augmented Generation

### ProblÃ¨me Ã  rÃ©soudre

**Sans RAG** :

- âŒ L'IA ne connaÃ®t que ce qu'elle a vu pendant son entraÃ®nement
- âŒ Pas d'accÃ¨s aux donnÃ©es privÃ©es (votre codebase, vos docs internes)
- âŒ Limite de contexte (128k-2M tokens max)

**Avec RAG** :

- âœ… L'IA peut accÃ©der Ã  des donnÃ©es externes
- âœ… Seulement les infos pertinentes sont rÃ©cupÃ©rÃ©es (Ã©conomie de tokens)
- âœ… DonnÃ©es Ã  jour (pas besoin de rÃ©entraÃ®ner le modÃ¨le)

---

### Architecture RAG

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Question   â”‚
â”‚  utilisateur â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 1. Embedding de la question
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Embedding    â”‚
â”‚   Model      â”‚
â”‚ (OpenAI Ada) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 2. Recherche sÃ©mantique
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Vector     â”‚
â”‚   Database   â”‚
â”‚ (ChromaDB,   â”‚
â”‚  Pinecone)   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 3. Documents pertinents
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     LLM      â”‚
â”‚  (GPT-4o,    â”‚
â”‚   Claude)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 4. RÃ©ponse gÃ©nÃ©rÃ©e
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   RÃ©ponse    â”‚
â”‚ Ã  l'user     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ã‰tapes RAG** :

1. **Indexation** (fait une fois) :

   - DÃ©couper les documents en chunks (morceaux)
   - GÃ©nÃ©rer des embeddings pour chaque chunk
   - Stocker dans une base vectorielle

2. **RÃ©cupÃ©ration** (Ã  chaque question) :

   - Transformer la question en embedding
   - Rechercher les chunks les plus similaires (similaritÃ© cosinus)
   - RÃ©cupÃ©rer les top-k chunks (ex: top 5)

3. **GÃ©nÃ©ration** :
   - Injecter les chunks pertinents dans le prompt
   - Le LLM gÃ©nÃ¨re la rÃ©ponse en se basant sur ces chunks

---

### Embeddings : ReprÃ©sentation vectorielle

**Qu'est-ce qu'un embedding ?**

- ReprÃ©sentation numÃ©rique d'un texte
- Vecteur de dimensions (ex: 1536 dimensions pour OpenAI Ada)
- Textes similaires â†’ vecteurs proches

**Exemple** :

```python
# Texte
text1 = "L'agence Ã©tait trÃ¨s professionnelle"
text2 = "Service de qualitÃ©, trÃ¨s pro"
text3 = "Le cafÃ© Ã©tait bon"

# Embeddings (vecteurs simplifiÃ©s Ã  3 dimensions pour l'exemple)
embedding1 = [0.8, 0.9, 0.1]  # Pro, qualitÃ©, non-cafÃ©
embedding2 = [0.85, 0.88, 0.05]  # TrÃ¨s proche de text1
embedding3 = [0.1, 0.2, 0.95]  # TrÃ¨s diffÃ©rent (parle de cafÃ©)

# SimilaritÃ© cosinus
sim(text1, text2) = 0.98  # TrÃ¨s similaire !
sim(text1, text3) = 0.23  # Peu similaire
```

**ModÃ¨les d'embeddings populaires** :

| ModÃ¨le                            | Dimensions | CoÃ»t (par 1M tokens) | Cas d'usage         |
| --------------------------------- | ---------- | -------------------- | ------------------- |
| **OpenAI text-embedding-3-small** | 1536       | $0.02                | GÃ©nÃ©ral, Ã©conomique |
| **OpenAI text-embedding-3-large** | 3072       | $0.13                | Haute prÃ©cision     |
| **Cohere embed-multilingual**     | 1024       | $0.10                | Multilingue         |

---

### Chunking : DÃ©couper intelligemment

**Pourquoi chunker ?**

- Embeddings limitÃ©s Ã  ~8000 tokens par chunk
- Chunks trop longs = perte de prÃ©cision sÃ©mantique
- Chunks trop courts = perte de contexte

**StratÃ©gies de chunking** :

| StratÃ©gie          | Taille           | Quand l'utiliser                           |
| ------------------ | ---------------- | ------------------------------------------ |
| **Fixed-size**     | 500 tokens       | Documents homogÃ¨nes (docs techniques)      |
| **Sentence-based** | Phrases entiÃ¨res | Texte naturel (articles, avis)             |
| **Semantic**       | Variable         | DÃ©coupage par section/paragraphe logique   |
| **Code-aware**     | Variable         | Code source (dÃ©couper par fonction/classe) |

**Exemple chunking d'avis** :

```python
# Avis original (trop long pour un seul embedding)
review = """
L'agence Immodvisor Paris 15 m'a accompagnÃ© pour l'achat de mon appartement.
Le conseiller Ã©tait trÃ¨s professionnel et Ã  l'Ã©coute.
Le processus a Ã©tÃ© rapide, seulement 2 mois entre la premiÃ¨re visite et la signature.
Cependant, quelques documents manquaient au dossier initial, ce qui a causÃ© un lÃ©ger retard.
Dans l'ensemble, je recommande vivement cette agence.
Note: 4/5
"""

# Chunking par phrase logique
chunks = [
    "L'agence Immodvisor Paris 15 m'a accompagnÃ© pour l'achat de mon appartement. Le conseiller Ã©tait trÃ¨s professionnel et Ã  l'Ã©coute.",
    "Le processus a Ã©tÃ© rapide, seulement 2 mois entre la premiÃ¨re visite et la signature.",
    "Cependant, quelques documents manquaient au dossier initial, ce qui a causÃ© un lÃ©ger retard.",
    "Dans l'ensemble, je recommande vivement cette agence. Note: 4/5"
]
```

---

## ğŸ› ï¸ Mise en pratique dans les exercices

### Exercice 1 : API Avis avec PACT-R

**Concepts appliquÃ©s** :

- âœ… **LLM** : Utiliser Claude Code pour gÃ©nÃ©rer code et tests
- âœ… **Tokens** : Optimiser les prompts pour Ã©conomiser des tokens
- âœ… **TempÃ©rature** : 0 pour gÃ©nÃ©ration de code dÃ©terministe
- âœ… **Prompting** : Structurer les demandes pour obtenir du bon code

---

### Exercice 2 : Serveur MCP + RAG

**Concepts appliquÃ©s** :

- âœ… **MCP** : CrÃ©er un serveur MCP exposant des tools
- âœ… **RAG** : Indexer des avis, rÃ©cupÃ©rer les plus pertinents
- âœ… **Embeddings** : GÃ©nÃ©rer avec OpenAI API
- âœ… **Chunking** : DÃ©couper les avis pour indexation

**Architecture complÃ¨te** :

```
Claude Code (client MCP)
    â†“
MCP Server (Node.js/Python)
    â†“
ChromaDB (base vectorielle)
    â†“
Embeddings OpenAI
    â†“
Avis Immodvisor (donnÃ©es)
```

---

## ğŸ’¡ Points clÃ©s Ã  retenir

### LLM & Tokens

- Les LLMs prÃ©disent la suite la plus probable
- Tokens = unitÃ© de mesure (4 car â‰ˆ 1 token)
- TempÃ©rature 0 pour code dÃ©terministe
- Contexte limitÃ© (128k-2M tokens selon modÃ¨le)

### Prompting efficace

- Structurer : Contexte â†’ TÃ¢che â†’ Contraintes â†’ Questions
- Role prompting : DÃ©finir le rÃ´le de l'IA
- Few-shot : Donner des exemples de bon code
- ItÃ©rations guidÃ©es : Valider Ã©tape par Ã©tape

### MCP

- Protocole pour connecter outils externes aux LLMs
- Architecture : LLM Client â†” MCP Server â†” Data/APIs
- Permet d'accÃ©der Ã  des donnÃ©es privÃ©es
- Exercice 2 = construire un serveur MCP

### RAG

- Retrieval Augmented Generation
- Permet d'accÃ©der Ã  des donnÃ©es non connues du LLM
- Architecture : Question â†’ Embedding â†’ Recherche vectorielle â†’ GÃ©nÃ©ration
- Chunks + Embeddings + SimilaritÃ© = RÃ©cupÃ©ration pertinente

---

## ğŸ“š Ressources pour approfondir

### Documentation officielle

- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)
- [Anthropic Claude Docs](https://docs.anthropic.com/)
- [Model Context Protocol](https://modelcontextprotocol.io/)

### Outils et frameworks

- [LangChain](https://www.langchain.com/) - Framework pour applications LLM
- [Langfuse](https://langfuse.com/) - ObservabilitÃ© LLM (traces, mÃ©triques)
- [ChromaDB](https://www.trychroma.com/) - Base de donnÃ©es vectorielle
- [Pinecone](https://www.pinecone.io/) - Vector database as a service

### Tutoriels

- [OpenAI Cookbook](https://cookbook.openai.com/) - Recettes pratiques
- [Anthropic Prompt Engineering Guide](https://docs.anthropic.com/en/docs/prompt-engineering)

---

**Prochaine session** : [Atelier : Construction de la charte d'utilisation](../charte/guidelines-vibe-coding.md)
